/**
 * Script de d√©bogage sp√©cifique pour la transcription
 * √Ä ex√©cuter dans la console d'Obsidian
 */

// Fonction pour tester le flux complet de transcription
async function debugTranscriptionFlow() {
    console.log('üîç D√©bogage du flux de transcription...');
    
    try {
        // 1. V√©rifier l'initialisation des providers
        console.log('\n1Ô∏è‚É£ V√©rification des providers...');
        
        // Importer les fonctions du syst√®me de providers
        const { getAllProvidersList, getTranscriberProvider, getSummarizerProvider } = await import('./src/providers/index.ts');
        
        const allProviders = getAllProvidersList();
        console.log('üìã Providers enregistr√©s:', allProviders);
        
        // 2. V√©rifier le provider de transcription s√©lectionn√©
        console.log('\n2Ô∏è‚É£ Test du provider de transcription...');
        
        try {
            const transcriber = getTranscriberProvider('openai-whisper');
            console.log('‚úÖ Provider OpenAI r√©cup√©r√©:', transcriber.name);
            
            // Test de sant√© du provider
            const health = await transcriber.check();
            console.log('üè• Sant√© du provider:', health);
            
            if (!health.ok) {
                console.error('‚ùå Provider OpenAI non op√©rationnel:', health.details);
                return;
            }
            
        } catch (error) {
            console.error('‚ùå Erreur lors de la r√©cup√©ration du provider:', error);
            return;
        }
        
        // 3. Tester la cr√©ation d'un Blob audio de test
        console.log('\n3Ô∏è‚É£ Test de cr√©ation d\'un Blob audio...');
        
        // Cr√©er un Blob audio de test (silence de 1 seconde)
        const sampleRate = 16000;
        const duration = 1; // 1 seconde
        const length = sampleRate * duration;
        const buffer = new ArrayBuffer(length * 2); // 16-bit
        const view = new DataView(buffer);
        
        // Remplir avec du silence (valeurs √† 0)
        for (let i = 0; i < length; i++) {
            view.setInt16(i * 2, 0, true); // little-endian
        }
        
        const audioBlob = new Blob([buffer], { type: 'audio/wav' });
        console.log('üéµ Blob audio cr√©√©:', {
            size: audioBlob.size,
            type: audioBlob.type,
            sizeKB: (audioBlob.size / 1024).toFixed(2)
        });
        
        // 4. Tester la transcription
        console.log('\n4Ô∏è‚É£ Test de transcription...');
        
        try {
            const transcriber = getTranscriberProvider('openai-whisper');
            console.log('üîÑ D√©but de la transcription...');
            
            const startTime = Date.now();
            const result = await transcriber.transcribe(audioBlob);
            const endTime = Date.now();
            
            console.log('‚úÖ Transcription r√©ussie:', {
                duration: `${endTime - startTime}ms`,
                text: result.text,
                language: result.lang,
                segments: result.segments?.length || 0
            });
            
        } catch (error) {
            console.error('‚ùå Erreur de transcription:', error);
            console.error('D√©tails de l\'erreur:', {
                name: error.name,
                message: error.message,
                stack: error.stack
            });
        }
        
    } catch (error) {
        console.error('üí• Erreur g√©n√©rale:', error);
    }
}

// Fonction pour tester la configuration des param√®tres
async function debugSettings() {
    console.log('\nüîß V√©rification de la configuration...');
    
    try {
        // Acc√©der aux param√®tres du plugin (si possible)
        const app = window.app;
        if (app && app.plugins) {
            const plugin = app.plugins.plugins['ai-voice-meeting-notes'];
            if (plugin) {
                console.log('‚öôÔ∏è Param√®tres du plugin:', plugin.settings);
                
                // V√©rifier la cl√© API
                if (plugin.settings.openaiApiKey) {
                    console.log('üîë Cl√© API OpenAI configur√©e:', plugin.settings.openaiApiKey.substring(0, 8) + '...');
                } else {
                    console.warn('‚ö†Ô∏è Cl√© API OpenAI non configur√©e');
                }
                
                // V√©rifier le provider s√©lectionn√©
                console.log('üéØ Provider de transcription:', plugin.settings.transcriberProvider);
                console.log('üìù Provider de r√©sum√©:', plugin.settings.summarizerProvider);
                
            } else {
                console.warn('‚ö†Ô∏è Plugin non trouv√©');
            }
        } else {
            console.warn('‚ö†Ô∏è Impossible d\'acc√©der aux param√®tres du plugin');
        }
    } catch (error) {
        console.error('‚ùå Erreur lors de la v√©rification des param√®tres:', error);
    }
}

// Fonction pour tester l'enregistrement audio
async function debugAudioRecording() {
    console.log('\nüéôÔ∏è Test de l\'enregistrement audio...');
    
    try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            console.error('‚ùå getUserMedia non support√©');
            return;
        }
        
        // Tester l'acc√®s au microphone
        const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                channelCount: 1,
                sampleRate: 16000,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });
        
        console.log('‚úÖ Acc√®s au microphone r√©ussi');
        
        // Tester MediaRecorder
        const supportedTypes = [
            'audio/webm;codecs=opus',
            'audio/mp4;codecs=mp4a.40.2',
            'audio/webm',
            'audio/mp4'
        ];
        
        const supportedType = supportedTypes.find(type => MediaRecorder.isTypeSupported(type));
        console.log('üéµ Format audio support√©:', supportedType);
        
        // Cr√©er un enregistrement de test
        const mediaRecorder = new MediaRecorder(stream, { mimeType: supportedType });
        const chunks = [];
        
        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                chunks.push(event.data);
            }
        };
        
        return new Promise((resolve) => {
            mediaRecorder.onstop = () => {
                const audioBlob = new Blob(chunks, { type: supportedType });
                console.log('üéµ Enregistrement de test cr√©√©:', {
                    size: audioBlob.size,
                    type: audioBlob.type,
                    chunks: chunks.length
                });
                
                // Nettoyer
                stream.getTracks().forEach(track => track.stop());
                resolve(audioBlob);
            };
            
            // Enregistrer pendant 1 seconde
            mediaRecorder.start();
            setTimeout(() => mediaRecorder.stop(), 1000);
        });
        
    } catch (error) {
        console.error('‚ùå Erreur lors du test audio:', error);
    }
}

// Fonction principale de d√©bogage
async function runFullDebug() {
    console.log('üöÄ D√©marrage du d√©bogage complet de la transcription...\n');
    
    await debugSettings();
    await debugAudioRecording();
    await debugTranscriptionFlow();
    
    console.log('\n‚úÖ D√©bogage termin√©');
}

// Exporter les fonctions
window.debugTranscriptionFlow = {
    runFullDebug,
    debugSettings,
    debugAudioRecording,
    debugTranscriptionFlow
};

console.log('üîß Script de d√©bogage de transcription charg√©. Utilisez:');
console.log('- debugTranscriptionFlow.runFullDebug() pour un d√©bogage complet');
console.log('- debugTranscriptionFlow.debugSettings() pour v√©rifier la config');
console.log('- debugTranscriptionFlow.debugAudioRecording() pour tester l\'audio');
console.log('- debugTranscriptionFlow.debugTranscriptionFlow() pour tester la transcription');
