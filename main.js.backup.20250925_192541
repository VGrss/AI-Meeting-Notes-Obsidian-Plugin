/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// main.ts
var main_exports = {};
__export(main_exports, {
  default: () => VoiceNotesPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian4 = require("obsidian");

// ui/RecordingModal.ts
var import_obsidian2 = require("obsidian");

// audio/VoiceRecorder.ts
var VoiceRecorder = class {
  constructor() {
    this.mediaRecorder = null;
    this.stream = null;
    this.chunks = [];
    // Audio compression settings for smaller file sizes
    this.AUDIO_SETTINGS = {
      mimeType: "audio/webm;codecs=opus",
      // Opus codec for better compression
      audioBitsPerSecond: 32e3
      // 32 kbps for good quality/size balance
    };
    // Chunking settings for long recordings
    this.CHUNK_DURATION_MS = 12e4;
    // 2 minutes per chunk
    this.isChunking = false;
    this.chunkStartTime = 0;
    // Fallback settings if primary format not supported
    this.FALLBACK_SETTINGS = [
      { mimeType: "audio/mp4;codecs=mp4a.40.2", audioBitsPerSecond: 32e3 },
      // AAC
      { mimeType: "audio/webm", audioBitsPerSecond: 32e3 },
      // WebM default
      { mimeType: "audio/mp4", audioBitsPerSecond: 32e3 }
      // MP4 default
    ];
  }
  getBestAudioSettings() {
    if (MediaRecorder.isTypeSupported(this.AUDIO_SETTINGS.mimeType)) {
      return this.AUDIO_SETTINGS;
    }
    for (const settings of this.FALLBACK_SETTINGS) {
      if (MediaRecorder.isTypeSupported(settings.mimeType)) {
        console.warn("Using fallback audio settings:", {
          function: "VoiceRecorder.getBestAudioSettings",
          selectedSettings: settings
        });
        return settings;
      }
    }
    console.warn("Using default audio settings (no compression):", {
      function: "VoiceRecorder.getBestAudioSettings",
      reason: "no_supported_formats"
    });
    return {};
  }
  async start() {
    try {
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        const error = new Error("getUserMedia not supported in this browser");
        console.error("getUserMedia not supported:", {
          function: "VoiceRecorder.start",
          reason: "getUserMedia_not_supported",
          userAgent: navigator.userAgent
        });
        throw error;
      }
      this.stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          // Mono audio
          sampleRate: 16e3,
          // 16kHz sample rate (adequate for speech)
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      const audioSettings = this.getBestAudioSettings();
      this.mediaRecorder = new MediaRecorder(this.stream, audioSettings);
      this.chunks = [];
      console.log("Recording started with optimized settings:", {
        function: "VoiceRecorder.start",
        audioSettings
      });
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.chunks.push(event.data);
        }
      };
      this.mediaRecorder.onerror = (event) => {
        const error = new Error(`MediaRecorder error: ${event.type}`);
        console.error("MediaRecorder error:", {
          function: "VoiceRecorder.mediaRecorder.onerror",
          eventType: event.type,
          error
        });
      };
      this.mediaRecorder.start(this.CHUNK_DURATION_MS);
      this.chunkStartTime = Date.now();
      console.log("Voice recording started successfully:", {
        function: "VoiceRecorder.start",
        chunkDurationMs: this.CHUNK_DURATION_MS
      });
    } catch (error) {
      let enhancedError;
      let errorContext = {
        function: "VoiceRecorder.start",
        userAgent: navigator.userAgent
      };
      if (error.name === "NotAllowedError") {
        enhancedError = new Error("Microphone access denied. Please allow microphone permissions.");
        errorContext.reason = "microphone_access_denied";
      } else if (error.name === "NotFoundError") {
        enhancedError = new Error("No microphone found. Please connect a microphone.");
        errorContext.reason = "no_microphone_found";
      } else {
        enhancedError = new Error(`Failed to access microphone: ${error.message}`);
        errorContext.reason = "microphone_access_failed";
        errorContext.originalError = error.message;
        errorContext.errorName = error.name;
      }
      console.error("Voice recording start error:", {
        error: enhancedError,
        context: errorContext
      });
      throw enhancedError;
    }
  }
  pause() {
    if (this.mediaRecorder && this.mediaRecorder.state === "recording") {
      this.mediaRecorder.pause();
    }
  }
  resume() {
    if (this.mediaRecorder && this.mediaRecorder.state === "paused") {
      this.mediaRecorder.resume();
    }
  }
  async stop() {
    return new Promise((resolve) => {
      if (this.mediaRecorder) {
        this.mediaRecorder.onstop = () => {
          const audioBlob = new Blob(this.chunks, { type: "audio/wav" });
          this.cleanup();
          resolve(audioBlob);
        };
        this.mediaRecorder.stop();
      }
    });
  }
  cleanup() {
    if (this.stream) {
      this.stream.getTracks().forEach((track) => track.stop());
      this.stream = null;
    }
    this.mediaRecorder = null;
    this.chunks = [];
  }
};

// ui/TranscriptModal.ts
var import_obsidian = require("obsidian");
var TranscriptModal = class extends import_obsidian.Modal {
  constructor(app, summarizer, transcript) {
    super(app);
    this.summary = "";
    this.summarizer = summarizer;
    this.transcript = transcript;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.createEl("h2", { text: "Transcript Results" });
    const transcriptContainer = contentEl.createDiv("transcript-container");
    transcriptContainer.createEl("h3", { text: "Raw Transcript" });
    const transcriptEl = transcriptContainer.createEl("textarea", {
      attr: {
        readonly: "true",
        rows: "10",
        cols: "50"
      }
    });
    transcriptEl.value = this.transcript;
    const summaryContainer = contentEl.createDiv("summary-container");
    summaryContainer.createEl("h3", { text: "AI Summary" });
    const summaryEl = summaryContainer.createEl("textarea", {
      attr: {
        readonly: "true",
        rows: "6",
        cols: "50",
        placeholder: 'Click "Generate Summary" to create AI summary'
      }
    });
    const actionsEl = contentEl.createDiv("actions");
    const summaryBtn = actionsEl.createEl("button", { text: "Generate Summary" });
    summaryBtn.onclick = async () => {
      summaryBtn.disabled = true;
      summaryBtn.textContent = "Generating...";
      try {
        this.summary = await this.generateSummary();
        summaryEl.value = this.summary;
      } catch (error) {
        new import_obsidian.Notice("Failed to generate summary: " + error.message);
      }
      summaryBtn.disabled = false;
      summaryBtn.textContent = "Generate Summary";
    };
    const insertBtn = actionsEl.createEl("button", { text: "Insert into Note" });
    insertBtn.onclick = () => this.insertIntoNote();
    const copyBtn = actionsEl.createEl("button", { text: "Copy to Clipboard" });
    copyBtn.onclick = () => this.copyToClipboard();
  }
  async generateSummary() {
    const result = await this.summarizer.summarize(this.transcript);
    return result.summary;
  }
  async insertIntoNote() {
    const activeView = this.app.workspace.getActiveViewOfType(import_obsidian.MarkdownView);
    if (activeView) {
      const editor = activeView.editor;
      const content = this.formatContent();
      editor.replaceSelection(content);
      new import_obsidian.Notice("Transcript inserted into current note");
    } else {
      try {
        await this.app.workspace.openLinkText("Voice Meeting Notes - " + new Date().toLocaleString(), "", true);
        await new Promise((resolve) => setTimeout(resolve, 200));
        const newActiveView = this.app.workspace.getActiveViewOfType(import_obsidian.MarkdownView);
        if (newActiveView) {
          const editor = newActiveView.editor;
          editor.setValue(this.formatContent());
          new import_obsidian.Notice("New note created with transcript");
        }
      } catch (error) {
        new import_obsidian.Notice("Failed to insert transcript: " + error.message);
      }
    }
    this.close();
  }
  copyToClipboard() {
    navigator.clipboard.writeText(this.formatContent());
    new import_obsidian.Notice("Transcript copied to clipboard");
  }
  formatContent() {
    let content = `## Voice Meeting Notes - ${new Date().toLocaleString()}

`;
    if (this.summary) {
      content += `### Summary
${this.summary}

`;
    }
    content += `### Raw Transcript
${this.transcript}`;
    return content;
  }
  onClose() {
    const { contentEl } = this;
    contentEl.empty();
  }
};

// src/providers/errors.ts
var ProviderError = class extends Error {
  constructor(code, message, options = {}) {
    super(message);
    this.name = "ProviderError";
    this.code = code;
    this.hint = options.hint;
    this.metadata = options.metadata;
    this.providerId = options.providerId;
    if (options.cause && options.cause.stack) {
      this.stack = options.cause.stack;
    }
  }
  /**
   * Convertit l'erreur en objet JSON pour le logging
   */
  toJSON() {
    return {
      name: this.name,
      code: this.code,
      message: this.message,
      hint: this.hint,
      providerId: this.providerId,
      metadata: this.metadata,
      stack: this.stack
    };
  }
  /**
   * Crée une erreur de configuration invalide
   */
  static configInvalid(message, hint, metadata) {
    return new ProviderError("CONFIG_INVALID" /* CONFIG_INVALID */, message, { hint, metadata });
  }
  /**
   * Crée une erreur de configuration manquante
   */
  static configMissing(key, providerId) {
    return new ProviderError(
      "CONFIG_MISSING" /* CONFIG_MISSING */,
      `Configuration manquante: ${key}`,
      {
        hint: `V\xE9rifiez que la configuration pour '${key}' est d\xE9finie`,
        metadata: { missingKey: key },
        providerId
      }
    );
  }
  /**
   * Crée une erreur de connexion
   */
  static connectionFailed(providerId, cause) {
    return new ProviderError(
      "CONNECTION_FAILED" /* CONNECTION_FAILED */,
      `Impossible de se connecter au provider ${providerId}`,
      {
        hint: "V\xE9rifiez votre connexion internet et la configuration du provider",
        providerId,
        cause
      }
    );
  }
  /**
   * Crée une erreur d'authentification
   */
  static authInvalid(providerId, hint) {
    return new ProviderError(
      "AUTH_INVALID" /* AUTH_INVALID */,
      `Authentification invalide pour le provider ${providerId}`,
      {
        hint: hint || "V\xE9rifiez votre cl\xE9 API ou vos identifiants",
        providerId
      }
    );
  }
  /**
   * Crée une erreur de quota dépassé
   */
  static quotaExceeded(providerId, limit) {
    return new ProviderError(
      "QUOTA_EXCEEDED" /* QUOTA_EXCEEDED */,
      `Quota d\xE9pass\xE9 pour le provider ${providerId}`,
      {
        hint: limit ? `Limite: ${limit}. Attendez ou mettez \xE0 niveau votre plan.` : "Votre quota mensuel est d\xE9pass\xE9",
        providerId,
        metadata: { limit }
      }
    );
  }
  /**
   * Crée une erreur de fichier non trouvé
   */
  static fileNotFound(filePath, providerId) {
    return new ProviderError(
      "FILE_NOT_FOUND" /* FILE_NOT_FOUND */,
      `Fichier non trouv\xE9: ${filePath}`,
      {
        hint: "V\xE9rifiez que le fichier existe et que le chemin est correct",
        metadata: { filePath },
        providerId
      }
    );
  }
  /**
   * Crée une erreur de format non supporté
   */
  static unsupportedFormat(format, providerId) {
    return new ProviderError(
      "UNSUPPORTED_FORMAT" /* UNSUPPORTED_FORMAT */,
      `Format non support\xE9: ${format}`,
      {
        hint: "V\xE9rifiez la liste des formats support\xE9s par ce provider",
        metadata: { format },
        providerId
      }
    );
  }
  /**
   * Crée une erreur de provider non trouvé
   */
  static providerNotFound(providerId, type) {
    return new ProviderError(
      "PROVIDER_NOT_FOUND" /* PROVIDER_NOT_FOUND */,
      `Provider non trouv\xE9: ${providerId}${type ? ` (type: ${type})` : ""}`,
      {
        hint: "V\xE9rifiez que le provider est bien enregistr\xE9 et que l'ID est correct",
        metadata: { providerId, type }
      }
    );
  }
  /**
   * Crée une erreur de traitement
   */
  static processingFailed(operation, providerId, cause) {
    return new ProviderError(
      "PROCESSING_FAILED" /* PROCESSING_FAILED */,
      `\xC9chec du traitement: ${operation}`,
      {
        hint: "V\xE9rifiez les logs pour plus de d\xE9tails sur l'erreur",
        metadata: { operation },
        providerId,
        cause
      }
    );
  }
};

// src/providers/registry.ts
var ProviderRegistry = class {
  constructor() {
    this.transcriberProviders = /* @__PURE__ */ new Map();
    this.summarizerProviders = /* @__PURE__ */ new Map();
  }
  /**
   * Enregistre un provider dans le registry
   */
  registerProvider(provider) {
    const { id } = provider;
    if (this.isProviderRegistered(id)) {
      throw ProviderError.providerNotFound(
        id,
        "Un provider avec cet ID est d\xE9j\xE0 enregistr\xE9"
      );
    }
    if (this.isTranscriberProvider(provider)) {
      this.transcriberProviders.set(id, provider);
    } else if (this.isSummarizerProvider(provider)) {
      this.summarizerProviders.set(id, provider);
    } else {
      throw new ProviderError(
        "INVALID_PROVIDER_TYPE" /* INVALID_PROVIDER_TYPE */,
        `Type de provider non support\xE9 pour ${id}`,
        { providerId: id }
      );
    }
  }
  /**
   * Récupère un provider de transcription par ID
   */
  getTranscriberProvider(id) {
    const provider = this.transcriberProviders.get(id);
    if (!provider) {
      throw ProviderError.providerNotFound(id, "transcriber");
    }
    return provider;
  }
  /**
   * Récupère un provider de résumé par ID
   */
  getSummarizerProvider(id) {
    const provider = this.summarizerProviders.get(id);
    if (!provider) {
      throw ProviderError.providerNotFound(id, "summarizer");
    }
    return provider;
  }
  /**
   * Récupère tous les providers d'un type donné
   */
  getAllProviders(type) {
    switch (type) {
      case "transcriber":
        return Array.from(this.transcriberProviders.values());
      case "summarizer":
        return Array.from(this.summarizerProviders.values());
      default:
        throw new ProviderError(
          "INVALID_PROVIDER_TYPE" /* INVALID_PROVIDER_TYPE */,
          `Type de provider non support\xE9: ${type}`
        );
    }
  }
  /**
   * Récupère tous les providers enregistrés
   */
  getAllProvidersList() {
    return [
      { type: "transcriber", providers: Array.from(this.transcriberProviders.values()) },
      { type: "summarizer", providers: Array.from(this.summarizerProviders.values()) }
    ];
  }
  /**
   * Vérifie si un provider est enregistré
   */
  isProviderRegistered(id) {
    return this.transcriberProviders.has(id) || this.summarizerProviders.has(id);
  }
  /**
   * Supprime un provider du registry
   */
  unregisterProvider(id) {
    let removed = false;
    if (this.transcriberProviders.delete(id))
      removed = true;
    if (this.summarizerProviders.delete(id))
      removed = true;
    return removed;
  }
  /**
   * Vide tous les providers du registry
   */
  clear() {
    this.transcriberProviders.clear();
    this.summarizerProviders.clear();
  }
  /**
   * Récupère le nombre de providers enregistrés par type
   */
  getProviderCount() {
    return {
      transcriber: this.transcriberProviders.size,
      summarizer: this.summarizerProviders.size
    };
  }
  /**
   * Récupère la liste des IDs de providers par type
   */
  getProviderIds(type) {
    switch (type) {
      case "transcriber":
        return Array.from(this.transcriberProviders.keys());
      case "summarizer":
        return Array.from(this.summarizerProviders.keys());
      default:
        throw new ProviderError(
          "INVALID_PROVIDER_TYPE" /* INVALID_PROVIDER_TYPE */,
          `Type de provider non support\xE9: ${type}`
        );
    }
  }
  /**
   * Vérifie si un provider est de type TranscriberProvider
   */
  isTranscriberProvider(provider) {
    return "transcribe" in provider && typeof provider.transcribe === "function";
  }
  /**
   * Vérifie si un provider est de type SummarizerProvider
   */
  isSummarizerProvider(provider) {
    return "summarize" in provider && typeof provider.summarize === "function";
  }
};
var providerRegistry = new ProviderRegistry();
function registerProvider(provider) {
  providerRegistry.registerProvider(provider);
}
function getTranscriberProvider(id) {
  return providerRegistry.getTranscriberProvider(id);
}
function getSummarizerProvider(id) {
  return providerRegistry.getSummarizerProvider(id);
}

// ui/RecordingModal.ts
var RecordingModal = class extends import_obsidian2.Modal {
  constructor(app, transcriberProviderId, summarizerProviderId) {
    super(app);
    this.recorder = null;
    this.isRecording = false;
    this.isPaused = false;
    this.recordingTime = 0;
    this.timeInterval = null;
    this.transcriberProviderId = transcriberProviderId;
    this.summarizerProviderId = summarizerProviderId;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.createEl("h2", { text: "AI Voice Meeting Notes" });
    const statusEl = contentEl.createDiv("recording-status");
    const timeEl = statusEl.createEl("div", {
      text: "00:00",
      cls: "recording-time"
    });
    const controlsEl = contentEl.createDiv("recording-controls");
    const startBtn = controlsEl.createEl("button", {
      text: "\u{1F399}\uFE0F Start Recording",
      cls: "start-btn"
    });
    const pauseBtn = controlsEl.createEl("button", {
      text: "\u23F8\uFE0F Pause",
      cls: "pause-btn",
      attr: { disabled: "true" }
    });
    const stopBtn = controlsEl.createEl("button", {
      text: "\u2705 Recording complete",
      cls: "stop-btn",
      attr: { disabled: "true" }
    });
    startBtn.onclick = () => this.startRecording(startBtn, pauseBtn, stopBtn, timeEl);
    pauseBtn.onclick = () => this.pauseRecording(pauseBtn, timeEl);
    stopBtn.onclick = () => this.stopRecording(startBtn, pauseBtn, stopBtn, timeEl);
  }
  async startRecording(startBtn, pauseBtn, stopBtn, timeEl) {
    if (!this.isRecording) {
      try {
        this.recorder = new VoiceRecorder();
        await this.recorder.start();
        this.isRecording = true;
        this.isPaused = false;
        startBtn.disabled = true;
        pauseBtn.disabled = false;
        stopBtn.disabled = false;
        startBtn.textContent = "Recording...";
        this.startTimer(timeEl);
        new import_obsidian2.Notice("Recording started");
      } catch (error) {
        new import_obsidian2.Notice("Failed to start recording: " + error.message);
      }
    }
  }
  pauseRecording(pauseBtn, timeEl) {
    if (this.recorder && !this.isPaused) {
      this.recorder.pause();
      this.isPaused = true;
      pauseBtn.textContent = "\u25B6\uFE0F Resume";
      this.stopTimer();
      new import_obsidian2.Notice("Recording paused");
    } else if (this.recorder && this.isPaused) {
      this.recorder.resume();
      this.isPaused = false;
      pauseBtn.textContent = "\u23F8\uFE0F Pause";
      this.startTimer(timeEl);
      new import_obsidian2.Notice("Recording resumed");
    }
  }
  async stopRecording(startBtn, pauseBtn, stopBtn, timeEl) {
    if (this.recorder) {
      this.stopTimer();
      this.recordingTime = 0;
      timeEl.textContent = "00:00";
      const audioBlob = await this.recorder.stop();
      this.isRecording = false;
      this.isPaused = false;
      startBtn.disabled = false;
      pauseBtn.disabled = true;
      stopBtn.disabled = true;
      startBtn.textContent = "Start Recording";
      pauseBtn.textContent = "\u23F8\uFE0F Pause";
      new import_obsidian2.Notice("Recording complete. Processing...");
      this.close();
      await this.processRecording(audioBlob);
    }
  }
  startTimer(timeEl) {
    this.stopTimer();
    this.timeInterval = setInterval(() => {
      this.recordingTime++;
      const minutes = Math.floor(this.recordingTime / 60);
      const seconds = this.recordingTime % 60;
      const display = `${minutes.toString().padStart(2, "0")}:${seconds.toString().padStart(2, "0")}`;
      timeEl.textContent = display;
    }, 1e3);
  }
  stopTimer() {
    if (this.timeInterval) {
      clearInterval(this.timeInterval);
      this.timeInterval = null;
    }
  }
  async processRecording(audioBlob) {
    try {
      const transcriber = getTranscriberProvider(this.transcriberProviderId);
      const summarizer = getSummarizerProvider(this.summarizerProviderId);
      const transcript = await transcriber.transcribe(audioBlob);
      new TranscriptModal(this.app, summarizer, transcript.text).open();
    } catch (error) {
      new import_obsidian2.Notice("Transcription failed: " + error.message);
    }
  }
  onClose() {
    const { contentEl } = this;
    contentEl.empty();
    this.stopTimer();
  }
};

// ui/RecordingView.ts
var import_obsidian3 = require("obsidian");
var RecordingView = class extends import_obsidian3.ItemView {
  constructor(leaf, transcriberProviderId, summarizerProviderId, recordings, onAddRecording, onSaveSettings) {
    super(leaf);
    this.recorder = null;
    this.isRecording = false;
    this.isPaused = false;
    this.recordingTime = 0;
    this.timeInterval = null;
    this.collapsedCards = {};
    this.activeTab = {};
    this.transcriberProviderId = transcriberProviderId;
    this.summarizerProviderId = summarizerProviderId;
    this.recordings = recordings;
    this.onAddRecording = onAddRecording;
    this.onSaveSettings = onSaveSettings;
  }
  getViewType() {
    return "voice-recording-view";
  }
  getDisplayText() {
    return "Voice Recording";
  }
  getIcon() {
    return "mic";
  }
  async onOpen() {
    const container = this.containerEl.children[1];
    container.empty();
    container.addClass("voice-recording-sidebar");
    const headerSection = container.createDiv("header-section");
    const headerTitle = headerSection.createEl("h4", {
      text: "AI Voice Recording",
      cls: "header-title"
    });
    const closeBtn = headerSection.createEl("button", {
      cls: "close-button",
      attr: {
        type: "button",
        "aria-label": "Close Panel"
      }
    });
    const closeSvg = closeBtn.createSvg("svg", {
      attr: {
        width: "16",
        height: "16",
        viewBox: "0 0 24 24",
        fill: "none"
      }
    });
    closeSvg.createSvg("path", {
      attr: {
        d: "M18 6L6 18M6 6L18 18",
        stroke: "currentColor",
        "stroke-width": "2",
        "stroke-linecap": "round",
        "stroke-linejoin": "round"
      }
    });
    const timerControlsSection = container.createDiv("timer-controls-section");
    const timeEl = timerControlsSection.createEl("div", {
      text: "00:00",
      cls: "timer-display"
    });
    const buttonStack = timerControlsSection.createDiv("button-stack");
    const startBtn = buttonStack.createEl("button", {
      text: "\u{1F399}\uFE0F Start Recording",
      cls: "primary-button start-recording-button",
      attr: {
        type: "button",
        "aria-label": "Start Recording"
      }
    });
    const pauseBtn = buttonStack.createEl("button", {
      text: "\u23F8\uFE0F Pause",
      cls: "secondary-button",
      attr: {
        disabled: "true",
        type: "button",
        "aria-label": "Pause Recording"
      }
    });
    const completeBtn = buttonStack.createEl("button", {
      text: "\u2705 Complete Recording",
      cls: "tertiary-button",
      attr: {
        disabled: "true",
        type: "button",
        "aria-label": "Complete Recording"
      }
    });
    const separator = container.createDiv("separator");
    const historySection = container.createDiv("history-section");
    const historyHeader = historySection.createDiv("history-header");
    historyHeader.createEl("h5", {
      text: "RECORDING HISTORY",
      cls: "history-title"
    });
    const historyListEl = historySection.createDiv("recordings-list");
    startBtn.onclick = () => this.toggleRecording(startBtn, pauseBtn, completeBtn, timeEl);
    pauseBtn.onclick = () => this.pauseRecording(pauseBtn, timeEl);
    completeBtn.onclick = () => this.completeRecording(startBtn, pauseBtn, completeBtn, historyListEl, timeEl);
    closeBtn.onclick = () => this.leaf.detach();
    this.refreshRecordingHistory(historyListEl);
  }
  async toggleRecording(startBtn, pauseBtn, completeBtn, timeEl) {
    if (!this.isRecording) {
      try {
        this.recorder = new VoiceRecorder();
        await this.recorder.start();
        this.isRecording = true;
        this.isPaused = false;
        startBtn.textContent = "\u{1F6D1} Stop Recording";
        startBtn.classList.add("recording-active");
        pauseBtn.disabled = false;
        completeBtn.disabled = false;
        this.startTimer(timeEl);
        new import_obsidian3.Notice("Recording started");
      } catch (error) {
        new import_obsidian3.Notice("Failed to start recording: " + error.message);
      }
    } else {
      new ConfirmDiscardModal(this.app, async () => {
        await this.discardRecording(startBtn, pauseBtn, completeBtn, timeEl);
      }).open();
    }
  }
  pauseRecording(pauseBtn, timeEl) {
    if (this.recorder && !this.isPaused) {
      this.recorder.pause();
      this.isPaused = true;
      pauseBtn.textContent = "\u25B6\uFE0F Resume";
      this.stopTimer();
      new import_obsidian3.Notice("Recording paused");
    } else if (this.recorder && this.isPaused) {
      this.recorder.resume();
      this.isPaused = false;
      pauseBtn.textContent = "\u23F8\uFE0F Pause";
      this.startTimer(timeEl);
      new import_obsidian3.Notice("Recording resumed");
    }
  }
  async discardRecording(startBtn, pauseBtn, completeBtn, timeEl) {
    if (this.recorder) {
      await this.recorder.stop();
      this.isRecording = false;
      this.isPaused = false;
      this.stopTimer();
      this.recordingTime = 0;
      timeEl.textContent = "00:00";
      startBtn.textContent = "\u{1F399}\uFE0F Start Recording";
      startBtn.classList.remove("recording-active");
      pauseBtn.disabled = true;
      pauseBtn.textContent = "\u23F8\uFE0F Pause";
      completeBtn.disabled = true;
      new import_obsidian3.Notice("Recording discarded");
    }
  }
  async completeRecording(startBtn, pauseBtn, completeBtn, historyListEl, timeEl) {
    if (this.recorder) {
      const recordingDuration = this.recordingTime;
      this.stopTimer();
      this.recordingTime = 0;
      timeEl.textContent = "00:00";
      const audioBlob = await this.recorder.stop();
      this.isRecording = false;
      this.isPaused = false;
      startBtn.textContent = "\u{1F399}\uFE0F Start Recording";
      startBtn.classList.remove("recording-active");
      pauseBtn.disabled = true;
      pauseBtn.textContent = "\u23F8\uFE0F Pause";
      completeBtn.disabled = true;
      const recordingId = Date.now().toString();
      const processingRecording = {
        id: recordingId,
        timestamp: new Date(),
        duration: recordingDuration,
        transcript: "\u23F3 Transcribing audio...",
        summary: "\u23F3 Processing will start after transcription...",
        topic: "\u23F3 Processing..."
      };
      this.onAddRecording(processingRecording);
      this.refreshRecordingHistory(historyListEl);
      new import_obsidian3.Notice("Recording complete. Processing...");
      try {
        const transcriber = getTranscriberProvider(this.transcriberProviderId);
        const summarizer = getSummarizerProvider(this.summarizerProviderId);
        const transcriptResult = await transcriber.transcribe(audioBlob);
        processingRecording.transcript = transcriptResult.text;
        processingRecording.summary = "\u23F3 Generating AI summary...";
        processingRecording.topic = "\u23F3 Generating topic...";
        this.refreshRecordingHistory(historyListEl);
        const [summaryResult, topicResult] = await Promise.all([
          summarizer.summarize(transcriptResult.text),
          summarizer.summarize(transcriptResult.text, {
            style: "brief",
            maxLength: 50
          })
        ]);
        processingRecording.summary = summaryResult.summary;
        processingRecording.topic = topicResult.summary;
        await this.onSaveSettings();
        this.refreshRecordingHistory(historyListEl);
        new import_obsidian3.Notice("Recording processed and saved!");
      } catch (error) {
        processingRecording.transcript = "\u274C Processing failed";
        processingRecording.summary = `\u274C Error: ${error.message}`;
        processingRecording.topic = "\u274C Failed";
        this.refreshRecordingHistory(historyListEl);
        new import_obsidian3.Notice("Processing failed: " + error.message);
      }
    }
  }
  startTimer(timeEl) {
    this.stopTimer();
    this.timeInterval = setInterval(() => {
      this.recordingTime++;
      const minutes = Math.floor(this.recordingTime / 60);
      const seconds = this.recordingTime % 60;
      const display = `${minutes.toString().padStart(2, "0")}:${seconds.toString().padStart(2, "0")}`;
      timeEl.textContent = display;
    }, 1e3);
  }
  stopTimer() {
    if (this.timeInterval) {
      clearInterval(this.timeInterval);
      this.timeInterval = null;
    }
  }
  refreshRecordingHistory(historyListEl) {
    historyListEl.empty();
    if (this.recordings.length === 0) {
      historyListEl.createEl("p", {
        text: "No recordings yet. Start recording to see them here!",
        cls: "empty-state"
      });
      return;
    }
    this.recordings.forEach((recording) => {
      this.createRecordingCard(historyListEl, recording);
    });
  }
  toggleCardCollapse(recordingId) {
    this.collapsedCards[recordingId] = !this.collapsedCards[recordingId];
    const historyListEl = this.containerEl.querySelector(".recordings-list");
    if (historyListEl) {
      this.refreshRecordingHistory(historyListEl);
    }
  }
  createRecordingCard(container, recording) {
    var _a, _b;
    if (this.collapsedCards[recording.id] === void 0) {
      this.collapsedCards[recording.id] = false;
    }
    if (this.activeTab[recording.id] === void 0) {
      this.activeTab[recording.id] = "summary";
    }
    const isCollapsed = this.collapsedCards[recording.id];
    const currentTab = this.activeTab[recording.id];
    const card = container.createDiv("recording-card group");
    const header = card.createDiv("card-header");
    header.onclick = () => this.toggleCardCollapse(recording.id);
    const chevron = header.createDiv("chevron-icon");
    const chevronSvg = chevron.createSvg("svg", {
      attr: {
        width: "12",
        height: "12",
        viewBox: "0 0 12 12",
        fill: "none"
      }
    });
    const pathData = isCollapsed ? "M4.5 3L7.5 6L4.5 9" : "M3 4.5L6 7.5L9 4.5";
    chevronSvg.createSvg("path", {
      attr: {
        d: pathData,
        stroke: "currentColor",
        "stroke-width": "1.5",
        "stroke-linecap": "round",
        "stroke-linejoin": "round"
      }
    });
    const title = header.createEl("span", {
      text: recording.topic || "Discussion",
      cls: `card-title ${((_a = recording.topic) == null ? void 0 : _a.includes("\u23F3")) ? "processing" : ""} ${((_b = recording.topic) == null ? void 0 : _b.includes("\u274C")) ? "error" : ""}`
    });
    const durationBadge = header.createEl("span", {
      text: `${Math.floor(recording.duration / 60)}:${(recording.duration % 60).toString().padStart(2, "0")}`,
      cls: "duration-badge"
    });
    if (!isCollapsed) {
      const content = card.createDiv("card-content");
      const tabNav = content.createDiv("tab-navigation");
      const tabContainer = tabNav.createDiv("tab-container");
      const summaryTab = tabContainer.createEl("button", {
        cls: `tab-button ${currentTab === "summary" ? "active" : ""}`
      });
      const summarySvg = summaryTab.createSvg("svg", {
        attr: {
          width: "10",
          height: "10",
          viewBox: "0 0 24 24",
          fill: "none"
        }
      });
      summarySvg.createSvg("path", {
        attr: {
          d: "M12 2L2 7L12 12L22 7L12 2Z",
          stroke: "currentColor",
          "stroke-width": "2",
          "stroke-linecap": "round",
          "stroke-linejoin": "round"
        }
      });
      summarySvg.createSvg("path", {
        attr: {
          d: "M2 17L12 22L22 17",
          stroke: "currentColor",
          "stroke-width": "2",
          "stroke-linecap": "round",
          "stroke-linejoin": "round"
        }
      });
      summarySvg.createSvg("path", {
        attr: {
          d: "M2 12L12 17L22 12",
          stroke: "currentColor",
          "stroke-width": "2",
          "stroke-linecap": "round",
          "stroke-linejoin": "round"
        }
      });
      summaryTab.createEl("span", { text: "AI Summary" });
      const transcriptTab = tabContainer.createEl("button", {
        cls: `tab-button ${currentTab === "transcript" ? "active" : ""}`
      });
      const transcriptSvg = transcriptTab.createSvg("svg", {
        attr: {
          width: "10",
          height: "10",
          viewBox: "0 0 24 24",
          fill: "none"
        }
      });
      transcriptSvg.createSvg("path", {
        attr: {
          d: "M14 2H6A2 2 0 0 0 4 4V20A2 2 0 0 0 6 22H18A2 2 0 0 0 20 20V8L14 2Z",
          stroke: "currentColor",
          "stroke-width": "2",
          "stroke-linecap": "round",
          "stroke-linejoin": "round"
        }
      });
      transcriptSvg.createSvg("path", {
        attr: {
          d: "M14 2V8H20",
          stroke: "currentColor",
          "stroke-width": "2",
          "stroke-linecap": "round",
          "stroke-linejoin": "round"
        }
      });
      transcriptTab.createEl("span", { text: "Transcript" });
      const contentArea = content.createDiv("content-area group");
      const textContent = contentArea.createDiv("text-content");
      const currentContent = currentTab === "summary" ? recording.summary : recording.transcript;
      const paragraphs = currentContent.split("\n\n");
      paragraphs.forEach((paragraph) => {
        if (paragraph.trim()) {
          const isProcessing = paragraph.includes("\u23F3");
          const isError = paragraph.includes("\u274C");
          textContent.createEl("p", {
            text: paragraph.trim(),
            cls: `content-paragraph ${isProcessing ? "processing" : ""} ${isError ? "error" : ""}`
          });
        }
      });
      const copyButton = contentArea.createEl("button", {
        cls: "copy-button",
        attr: { title: `Copy ${currentTab} to clipboard` }
      });
      copyButton.textContent = "\u{1F4CB}";
      summaryTab.onclick = (e) => {
        e.stopPropagation();
        this.activeTab[recording.id] = "summary";
        this.refreshRecordingHistory(container);
      };
      transcriptTab.onclick = (e) => {
        e.stopPropagation();
        this.activeTab[recording.id] = "transcript";
        this.refreshRecordingHistory(container);
      };
      copyButton.onclick = (e) => {
        e.stopPropagation();
        const contentToCopy = currentTab === "summary" ? recording.summary : recording.transcript;
        navigator.clipboard.writeText(contentToCopy);
        new import_obsidian3.Notice(`${currentTab === "summary" ? "Summary" : "Transcript"} copied to clipboard`);
      };
    }
  }
  async onClose() {
    if (this.recorder) {
      await this.recorder.stop();
    }
    this.stopTimer();
  }
};
var ConfirmDiscardModal = class extends import_obsidian3.Modal {
  constructor(app, onConfirm) {
    super(app);
    this.onConfirm = onConfirm;
  }
  onOpen() {
    const { contentEl } = this;
    contentEl.createEl("h2", { text: "\u{1F6D1} Discard Recording?" });
    const warningEl = contentEl.createDiv("warning-message");
    warningEl.createEl("p", {
      text: "Are you sure you want to stop and discard this recording?"
    });
    warningEl.createEl("p", {
      text: "This action cannot be undone and the audio will be lost permanently.",
      cls: "warning-text"
    });
    const buttonsEl = contentEl.createDiv("modal-buttons");
    const cancelBtn = buttonsEl.createEl("button", {
      text: "Cancel",
      cls: "modal-button-secondary"
    });
    const discardBtn = buttonsEl.createEl("button", {
      text: "\u{1F6D1} Yes, Discard Recording",
      cls: "modal-button-danger"
    });
    cancelBtn.onclick = () => this.close();
    discardBtn.onclick = () => {
      this.onConfirm();
      this.close();
    };
  }
  onClose() {
    const { contentEl } = this;
    contentEl.empty();
  }
};

// src/providers/openai/OpenAITranscriber.ts
var OpenAITranscriber = class {
  // 20MB recommended limit
  constructor(apiKey) {
    this.id = "openai-whisper";
    this.name = "OpenAI Whisper";
    this.type = "cloud";
    // OpenAI Whisper limits: 25MB max file size
    this.MAX_FILE_SIZE = 25 * 1024 * 1024;
    // 25MB in bytes
    this.RECOMMENDED_MAX_SIZE = 20 * 1024 * 1024;
    this.apiKey = apiKey;
  }
  async check() {
    if (!this.apiKey) {
      return {
        ok: false,
        details: "Cl\xE9 API OpenAI manquante"
      };
    }
    try {
      const response = await fetch("https://api.openai.com/v1/models", {
        method: "GET",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`
        }
      });
      if (!response.ok) {
        if (response.status === 401) {
          return {
            ok: false,
            details: "Cl\xE9 API OpenAI invalide"
          };
        }
        return {
          ok: false,
          details: `Erreur de connexion: ${response.status} ${response.statusText}`
        };
      }
      return {
        ok: true,
        details: "OpenAI Whisper disponible",
        capabilities: ["transcription", "multi-language", "segments"]
      };
    } catch (error) {
      return {
        ok: false,
        details: `Erreur de connexion: ${error.message}`
      };
    }
  }
  async transcribe(audioPath, opts) {
    var _a, _b;
    try {
      if (!this.apiKey) {
        throw ProviderError.authInvalid(this.id, "Cl\xE9 API OpenAI requise");
      }
      const audioBlob = audioPath;
      const sizeCheck = this.checkFileSize(audioBlob);
      const sizeMB = (audioBlob.size / (1024 * 1024)).toFixed(1);
      if (!sizeCheck.canUpload) {
        throw ProviderError.fileNotFound(
          `Fichier audio trop volumineux (${sizeMB}MB)`,
          this.id
        );
      }
      if (sizeCheck.message) {
        console.warn("Large file upload attempt:", {
          function: "OpenAITranscriber.transcribe",
          fileSizeMB: sizeMB,
          message: sizeCheck.message,
          recommendation: sizeCheck.recommendation
        });
      }
      const formData = new FormData();
      formData.append("file", audioBlob, "recording.wav");
      formData.append("model", "whisper-1");
      if (opts == null ? void 0 : opts.language) {
        formData.append("language", opts.language);
      }
      const response = await fetch("https://api.openai.com/v1/audio/transcriptions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`
        },
        body: formData
      });
      if (!response.ok) {
        const sizeMB2 = (audioBlob.size / (1024 * 1024)).toFixed(1);
        let errorMessage;
        if (response.status === 413) {
          errorMessage = `Audio file is too large to transcribe (${sizeMB2}MB).

Solutions:
\u2022 Record shorter segments (under 10 minutes)
\u2022 Use lower quality settings in your browser
\u2022 Break long recordings into smaller parts`;
        } else if (response.status === 400) {
          errorMessage = `Audio format not supported or file corrupted.

Try:
\u2022 Re-recording with different settings
\u2022 Ensuring your microphone works properly`;
        } else if (response.status === 429) {
          errorMessage = `Rate limit exceeded. Please wait a moment before trying again.`;
        } else if (response.status >= 500) {
          errorMessage = `OpenAI service is temporarily unavailable (${response.status}).

Please try again in a few minutes.`;
        } else {
          errorMessage = `Transcription failed (${response.status}): ${response.statusText}`;
        }
        throw new ProviderError(
          "PROCESSING_FAILED" /* PROCESSING_FAILED */,
          errorMessage,
          {
            providerId: this.id,
            metadata: {
              httpStatus: response.status,
              responseText: response.statusText,
              fileSizeMB: sizeMB2
            }
          }
        );
      }
      const result = await response.json();
      console.log("Audio transcription completed successfully:", {
        function: "OpenAITranscriber.transcribe",
        audioBlobSize: audioBlob.size,
        transcriptLength: ((_a = result.text) == null ? void 0 : _a.length) || 0
      });
      return {
        text: result.text,
        lang: result.language,
        segments: ((_b = result.segments) == null ? void 0 : _b.map((seg) => ({
          text: seg.text,
          start: seg.start,
          end: seg.end,
          confidence: seg.avg_logprob
        }))) || [],
        metadata: {
          duration: result.duration,
          model: "whisper-1",
          processingTime: Date.now()
          // Approximation
        }
      };
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      console.error("Unexpected error in transcription:", {
        function: "OpenAITranscriber.transcribe",
        audioPath,
        errorType: "unexpected",
        error
      });
      throw ProviderError.processingFailed(
        "Transcription OpenAI",
        this.id,
        error
      );
    }
  }
  checkFileSize(audioBlob) {
    const sizeMB = (audioBlob.size / (1024 * 1024)).toFixed(1);
    if (audioBlob.size > this.MAX_FILE_SIZE) {
      return {
        canUpload: false,
        message: `Audio file is too large (${sizeMB}MB). OpenAI Whisper has a 25MB limit.`,
        recommendation: "Try recording shorter segments (under 10 minutes) or use lower quality settings."
      };
    }
    if (audioBlob.size > this.RECOMMENDED_MAX_SIZE) {
      return {
        canUpload: true,
        message: `Audio file is large (${sizeMB}MB). This may take longer to process.`,
        recommendation: "For faster processing, consider shorter recordings."
      };
    }
    return { canUpload: true };
  }
};

// src/providers/openai/OpenAISummarizer.ts
var OpenAISummarizer = class {
  constructor(apiKey, customSummaryPrompt) {
    this.id = "openai-gpt4o";
    this.name = "OpenAI GPT-4o";
    this.type = "cloud";
    this.apiKey = apiKey;
    this.customSummaryPrompt = customSummaryPrompt;
  }
  async check() {
    if (!this.apiKey) {
      return {
        ok: false,
        details: "Cl\xE9 API OpenAI manquante"
      };
    }
    try {
      const response = await fetch("https://api.openai.com/v1/models", {
        method: "GET",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`
        }
      });
      if (!response.ok) {
        if (response.status === 401) {
          return {
            ok: false,
            details: "Cl\xE9 API OpenAI invalide"
          };
        }
        return {
          ok: false,
          details: `Erreur de connexion: ${response.status} ${response.statusText}`
        };
      }
      return {
        ok: true,
        details: "OpenAI GPT-4o disponible",
        capabilities: ["summarization", "multi-language", "custom-prompts"]
      };
    } catch (error) {
      return {
        ok: false,
        details: `Erreur de connexion: ${error.message}`
      };
    }
  }
  async summarize(text, opts) {
    var _a;
    try {
      if (!this.apiKey) {
        throw ProviderError.authInvalid(this.id, "Cl\xE9 API OpenAI requise");
      }
      let processedText = text;
      const maxTokensForContext = 12e3;
      const avgCharsPerToken = 4;
      const maxCharsForContext = maxTokensForContext * avgCharsPerToken;
      if (text.length > maxCharsForContext) {
        const firstPart = text.substring(0, maxCharsForContext * 0.4);
        const lastPart = text.substring(text.length - maxCharsForContext * 0.4);
        const middlePart = text.substring(
          Math.floor(text.length * 0.4),
          Math.floor(text.length * 0.6)
        ).substring(0, maxCharsForContext * 0.2);
        processedText = `${firstPart}

[...MIDDLE SECTION SUMMARY...]
${middlePart}

[...CONTINUED...]
${lastPart}`;
        console.warn("Long transcript truncated for summary:", {
          function: "OpenAISummarizer.summarize",
          originalLength: text.length,
          processedLength: processedText.length,
          truncationRatio: (processedText.length / text.length).toFixed(2)
        });
      }
      const prompt = (opts == null ? void 0 : opts.customPrompt) || this.customSummaryPrompt;
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "gpt-4o",
          messages: [{
            role: "user",
            content: `${prompt}

**Transcript:**
${processedText}`
          }],
          max_tokens: (opts == null ? void 0 : opts.maxLength) || 2e3,
          temperature: (opts == null ? void 0 : opts.style) === "detailed" ? 0.3 : 0.1
        })
      });
      if (!response.ok) {
        throw new ProviderError(
          "PROCESSING_FAILED" /* PROCESSING_FAILED */,
          `Summary generation failed: ${response.statusText}`,
          {
            providerId: this.id,
            metadata: {
              httpStatus: response.status,
              responseText: response.statusText,
              textLength: text.length
            }
          }
        );
      }
      const result = await response.json();
      const summary = result.choices[0].message.content;
      console.log("AI summary generated successfully:", {
        function: "OpenAISummarizer.summarize",
        originalTextLength: text.length,
        processedTextLength: processedText.length,
        summaryLength: (summary == null ? void 0 : summary.length) || 0,
        wasTruncated: text.length !== processedText.length
      });
      return {
        summary,
        tokens: (_a = result.usage) == null ? void 0 : _a.total_tokens,
        metadata: {
          originalLength: text.length,
          compressionRatio: summary.length / text.length,
          model: "gpt-4o",
          processingTime: Date.now()
          // Approximation
        }
      };
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      console.error("Unexpected error in summarization:", {
        function: "OpenAISummarizer.summarize",
        textLength: text.length,
        errorType: "unexpected",
        error
      });
      throw ProviderError.processingFailed(
        "R\xE9sum\xE9 OpenAI",
        this.id,
        error
      );
    }
  }
};

// main.ts
var DEFAULT_SUMMARY_PROMPT = `You are analyzing a voice recording transcript from a meeting or discussion. Please provide a comprehensive summary using the EXACT SAME LANGUAGE as the transcript (if transcript is in French, respond in French; if in Spanish, respond in Spanish, etc.).

Structure your response with these sections:

1. **Main Topics Discussed**: What were the primary subjects covered?
2. **Key Points**: The most important information shared
3. **Decisions Made**: Any conclusions or agreements reached
4. **Action Items**: Tasks or next steps identified (if any)
5. **Context & Insights**: Important context or insights that emerged

CRITICAL: Your entire response must be in the same language as the transcript. Do not translate or use English if the transcript is in another language.`;
var DEFAULT_SETTINGS = {
  openaiApiKey: "",
  glitchTipDsn: "https://fc4c4cf2c55b4aaaa076954be7e02814@app.glitchtip.com/12695",
  customSummaryPrompt: DEFAULT_SUMMARY_PROMPT,
  // Provider settings
  transcriberProvider: "openai-whisper",
  summarizerProvider: "openai-gpt4o",
  // Local provider configurations
  localProviders: {
    ollama: {
      host: "localhost",
      port: 11434,
      model: "mistral:7b"
    },
    whispercpp: {
      binaryPath: "",
      modelPath: "",
      extraArgs: []
    },
    fasterwhisper: {
      pythonPath: "python",
      modelName: "small"
    }
  }
};
var RECORDING_VIEW_TYPE = "voice-recording-view";
var VoiceNotesPlugin = class extends import_obsidian4.Plugin {
  constructor() {
    super(...arguments);
    this.recordings = [];
  }
  async onload() {
    await this.loadSettings();
    this.initializeProviders();
    this.addRibbonIcon("mic", "Open Voice Recording Panel", (evt) => {
      this.activateRecordingView();
    });
    this.addCommand({
      id: "toggle-recording-panel",
      name: "Toggle Voice Recording Panel",
      callback: () => {
        this.toggleRecordingView();
      }
    });
    this.addCommand({
      id: "start-recording-modal",
      name: "Start Voice Recording (Modal)",
      callback: () => {
        this.openRecordingModal();
      }
    });
    this.registerView(
      RECORDING_VIEW_TYPE,
      (leaf) => new RecordingView(
        leaf,
        this.settings.transcriberProvider,
        this.settings.summarizerProvider,
        this.recordings,
        (recording) => this.addRecording(recording),
        () => this.saveSettings()
      )
    );
    this.statusBarItem = this.addStatusBarItem();
    this.statusBarItem.setText("\u{1F399}\uFE0F Recording");
    this.statusBarItem.addClass("mod-clickable");
    this.statusBarItem.onClickEvent(() => {
      this.toggleRecordingView();
    });
    this.statusBarItem.setAttribute("title", "Toggle Voice Recording Panel");
    this.addSettingTab(new VoiceNotesSettingTab(this.app, this));
  }
  onunload() {
  }
  openRecordingModal() {
    new RecordingModal(this.app, this.settings.transcriberProvider, this.settings.summarizerProvider).open();
  }
  async activateRecordingView() {
    const { workspace } = this.app;
    let leaf = null;
    const leaves = workspace.getLeavesOfType(RECORDING_VIEW_TYPE);
    if (leaves.length > 0) {
      leaf = leaves[0];
    } else {
      leaf = workspace.getRightLeaf(false);
      await (leaf == null ? void 0 : leaf.setViewState({ type: RECORDING_VIEW_TYPE, active: true }));
    }
    if (leaf) {
      workspace.revealLeaf(leaf);
    }
  }
  async toggleRecordingView() {
    const { workspace } = this.app;
    const leaves = workspace.getLeavesOfType(RECORDING_VIEW_TYPE);
    if (leaves.length > 0) {
      leaves[0].detach();
    } else {
      await this.activateRecordingView();
    }
  }
  async loadSettings() {
    const data = await this.loadData() || {};
    this.settings = Object.assign({}, DEFAULT_SETTINGS, data.settings || {});
    this.recordings = data.recordings || [];
  }
  async saveSettings() {
    await this.saveData({ settings: this.settings, recordings: this.recordings });
  }
  addRecording(recording) {
    this.recordings.unshift(recording);
    this.saveSettings();
  }
  /**
   * Initialise tous les providers disponibles
   */
  initializeProviders() {
    const openaiTranscriber = new OpenAITranscriber(this.settings.openaiApiKey);
    const openaiSummarizer = new OpenAISummarizer(this.settings.openaiApiKey, this.settings.customSummaryPrompt);
    registerProvider(openaiTranscriber);
    registerProvider(openaiSummarizer);
  }
};
var VoiceNotesSettingTab = class extends import_obsidian4.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    containerEl.createEl("h2", { text: "AI Voice Meeting Notes Settings" });
    containerEl.createEl("p", {
      text: "Configure your AI-powered voice note-taking plugin. Choose your transcription and summary providers according to your needs.",
      cls: "setting-description"
    });
    containerEl.createEl("h3", { text: "\u{1F916} Provider Configuration" });
    containerEl.createEl("p", {
      text: "Select providers for audio transcription and AI summarization. OpenAI API key is only required for OpenAI cloud providers.",
      cls: "setting-description"
    });
    new import_obsidian4.Setting(containerEl).setName("Transcription Provider").setDesc("Choose the audio transcription service").addDropdown((dropdown) => {
      dropdown.addOption("openai-whisper", "OpenAI Whisper (Cloud)").addOption("whispercpp", "Whisper.cpp (Local)").addOption("fasterwhisper", "FasterWhisper (Local)").setValue(this.plugin.settings.transcriberProvider).onChange(async (value) => {
        this.plugin.settings.transcriberProvider = value;
        await this.plugin.saveSettings();
        this.display();
      });
    });
    new import_obsidian4.Setting(containerEl).setName("Summary Provider").setDesc("Choose the AI summarization service").addDropdown((dropdown) => {
      dropdown.addOption("openai-gpt4o", "OpenAI GPT-4o (Cloud)").addOption("ollama", "Ollama (Local)").addOption("gpt4all", "GPT4All (Local)").setValue(this.plugin.settings.summarizerProvider).onChange(async (value) => {
        this.plugin.settings.summarizerProvider = value;
        await this.plugin.saveSettings();
        this.display();
      });
    });
    const needsOpenAI = this.plugin.settings.transcriberProvider === "openai-whisper" || this.plugin.settings.summarizerProvider === "openai-gpt4o";
    if (needsOpenAI) {
      new import_obsidian4.Setting(containerEl).setName("OpenAI API Key").setDesc("Required for OpenAI providers (Whisper and/or GPT-4o)").addText((text) => {
        text.setPlaceholder("Enter your OpenAI API key");
        if (this.plugin.settings.openaiApiKey) {
          text.setValue("*".repeat(this.plugin.settings.openaiApiKey.length));
        } else {
          text.setValue("");
        }
        text.inputEl.type = "password";
        text.onChange(async (value) => {
          if (value !== "*".repeat(this.plugin.settings.openaiApiKey.length)) {
            this.plugin.settings.openaiApiKey = value;
            await this.plugin.saveSettings();
          }
        });
      }).addExtraButton((button) => button.setIcon("external-link").setTooltip("Get OpenAI API Key").onClick(() => {
        window.open("https://platform.openai.com/api-keys", "_blank");
      }));
      containerEl.createEl("p", {
        text: "\u{1F4A1} Need an API key? Visit the OpenAI platform above to create your account and get your API key.",
        cls: "help-text"
      });
    }
    if (this.plugin.settings.transcriberProvider === "whispercpp" || this.plugin.settings.transcriberProvider === "fasterwhisper" || this.plugin.settings.summarizerProvider === "ollama" || this.plugin.settings.summarizerProvider === "gpt4all") {
      containerEl.createEl("h4", { text: "Local Providers Configuration" });
      if (this.plugin.settings.summarizerProvider === "ollama") {
        containerEl.createEl("h5", { text: "Ollama" });
        new import_obsidian4.Setting(containerEl).setName("Ollama Host").setDesc("Ollama server address").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.ollama.host);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.ollama.host = value;
            await this.plugin.saveSettings();
          });
        });
        new import_obsidian4.Setting(containerEl).setName("Ollama Port").setDesc("Ollama server port").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.ollama.port.toString());
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.ollama.port = parseInt(value) || 11434;
            await this.plugin.saveSettings();
          });
        });
        new import_obsidian4.Setting(containerEl).setName("Ollama Model").setDesc("Model name to use").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.ollama.model);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.ollama.model = value;
            await this.plugin.saveSettings();
          });
        });
      }
      if (this.plugin.settings.transcriberProvider === "whispercpp") {
        containerEl.createEl("h5", { text: "WhisperCpp" });
        new import_obsidian4.Setting(containerEl).setName("WhisperCpp Binary Path").setDesc("Path to whisper.cpp executable").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.whispercpp.binaryPath);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.whispercpp.binaryPath = value;
            await this.plugin.saveSettings();
          });
        });
        new import_obsidian4.Setting(containerEl).setName("WhisperCpp Model Path").setDesc("Path to model file").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.whispercpp.modelPath);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.whispercpp.modelPath = value;
            await this.plugin.saveSettings();
          });
        });
      }
      if (this.plugin.settings.transcriberProvider === "fasterwhisper") {
        containerEl.createEl("h5", { text: "FasterWhisper" });
        new import_obsidian4.Setting(containerEl).setName("Python Path").setDesc("Path to Python executable").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.fasterwhisper.pythonPath);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.fasterwhisper.pythonPath = value;
            await this.plugin.saveSettings();
          });
        });
        new import_obsidian4.Setting(containerEl).setName("Model Name").setDesc("FasterWhisper model name to use").addText((text) => {
          text.setValue(this.plugin.settings.localProviders.fasterwhisper.modelName);
          text.onChange(async (value) => {
            this.plugin.settings.localProviders.fasterwhisper.modelName = value;
            await this.plugin.saveSettings();
          });
        });
      }
    }
    containerEl.createEl("h3", { text: "\u{1F4DD} AI Customization" });
    containerEl.createEl("p", {
      text: "Customize the prompt used to generate summaries of your voice recordings. This allows you to adapt the output format and focus to your specific needs.",
      cls: "setting-description"
    });
    new import_obsidian4.Setting(containerEl).setName("Custom Summary Prompt").setDesc("Customize the prompt sent to AI for generating summaries").addTextArea((text) => {
      text.setPlaceholder("Enter your custom summary prompt...");
      text.setValue(this.plugin.settings.customSummaryPrompt || DEFAULT_SUMMARY_PROMPT);
      text.inputEl.rows = 12;
      text.inputEl.addClass("custom-summary-prompt-textarea");
      text.onChange(async (value) => {
        this.plugin.settings.customSummaryPrompt = value;
        await this.plugin.saveSettings();
      });
    }).addExtraButton((button) => button.setIcon("reset").setTooltip("Reset to Default Prompt").onClick(async () => {
      this.plugin.settings.customSummaryPrompt = DEFAULT_SUMMARY_PROMPT;
      await this.plugin.saveSettings();
      this.display();
    }));
    containerEl.createEl("p", {
      text: '\u{1F4A1} The prompt should include instructions for the AI on how to analyze and summarize voice recordings. Use "**Transcript:**" as a placeholder where the actual transcript will be inserted.',
      cls: "help-text"
    });
  }
};
